Etapa 02:
Para esta etapa, a arquitetura do agente evolui para um modelo reativo baseado em modelo. O robô agora é dotado de um estado interno, uma memória, que permite construir um mapa do ambiente à medida que o explora. O objetivo central é visitar o maior número possível de células do grid, evitando repetições e contornando obstáculos de forma eficiente.
A memória do agente (memoria_robo) é implementada como um grafo, no qual cada célula do ambiente corresponde a um nó. Cada nó armazena um status:
nao_visitado, visitado, prioridade ou atual. Essa estrutura de dados é fundamental para a tomada de decisão, permitindo que o agente diferencie áreas conhecidas de desconhecidas.
A estratégia de exploração é hierárquica e busca maximizar a eficiência, minimizando passos redundantes. A lógica principal opera da seguinte forma:
Exploração Local Prioritária: O agente primeiro tenta se mover para uma célula adjacente que ainda não foi visitada. A abordagem de "olhar para as direções" segue uma ordem predefinida, otimizada para o mapa padrão disponibilizado na disciplina, garantindo uma varredura sistemática do seu entorno imediato.
Busca por Novos Territórios: Quando o robô se encontra em uma posição onde todos os vizinhos já foram visitados, ele consulta sua memória (o grafo) para encontrar o nó nao_visitado ou com status prioridade mais próximo. Essa busca é realizada por meio de um algoritmo de busca em largura (BFS), que garante a descoberta do caminho mais curto em um ambiente não ponderado.
Navegação e Atualização: Uma vez identificado o próximo destino, o agente utiliza novamente um algoritmo de busca (implementado na função encontrar_caminho_para) para traçar a rota mais curta até ele, atravessando células já conhecidas se necessário. Ao se mover, o agente atualiza o status dos nós em seu mapa interno, incrementando um contador de
 espacos_repetidos sempre que pisa em uma célula já visitada, o que serve como métrica de eficiência.
A introdução de obstáculos é tratada de forma elegante pelo modelo de grafo: as células correspondentes aos obstáculos simplesmente não são adicionadas como nós acessíveis no mapa do agente, fazendo com que os algoritmos de busca de caminho naturalmente os contornem.
É válido notar que testamos outras heurísticas de exploração que, em teoria, poderiam oferecer melhor desempenho em mapas com configurações de obstáculos aleatórias. Contudo, nos testes realizados com o mapa específico proposto pelo professor, a abordagem implementada se mostrou mais robusta e eficiente, garantindo 100% de completude da exploração com um número reduzido de movimentos redundantes. O comportamento do agente é, portanto, generalizável a diferentes layouts, embora tenha sido adaptado para o cenário da atividade.
